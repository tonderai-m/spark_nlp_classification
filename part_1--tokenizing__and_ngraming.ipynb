{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configspark import create_session, read_data\n",
    "import pyspark.sql.functions as f \n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Objective is to classify Reviews based on ratings to be honest you probably need to do chisquared first to see uniqueness of words per class\n",
    "  but that doesn't stop the fun this project is to show how to pipeline the data and some basic cleaning not trying to get the best model \n",
    "* The note books are separated in parts because I want to exime inside the pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configspark\n",
    "* I was being lazy and I added the spark config function and also the read data it's all in the configspark.py at list I added a schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data to train and validate \n",
    "* when you fit a pipeline I line to transform a different dataset than the one I have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Split the data using randomSplit()\n",
    "train_data, test_data, validation_data = df.randomSplit([train_ratio, test_ratio, validation_ratio], seed=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[review: string, rating: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelining\n",
    "* like it says pipeline it's a chain the previous transformation is linked to the next transformation order of excecution is maintained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to concatenate feature columns into one column called text\n",
    "# featureConcat = FeatureConcatenator(outputCols = [\"text\"], inputCols = [target_col])\n",
    "\n",
    "# Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. \n",
    "# The DocumentAssembler can read either a String column or an Array[String]\n",
    "documentAssembler = DocumentAssembler().setInputCol(\"review\").setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizes raw text in document type columns into TokenizedSentence\n",
    "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
    "\n",
    "# Annotator that cleans out tokens.\n",
    "# Remove white space\n",
    "normalizer = Normalizer().setInputCols(\"token\").setOutputCol(\"normalized\").setLowercase(True).setCleanupPatterns([\"[^\\w\\s]\"])\n",
    "\n",
    "# Remove years (integers starting with 19XX or 20XX)\n",
    "removeYear = Normalizer().setInputCols([\"normalized\"]).setOutputCol(\"remove_year\").setCleanupPatterns([\"(?:(?:19|20)\\d\\d)\"])\n",
    "\n",
    "# Find lemmas out of words with the objective of returning a base dictionary word\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols(\"remove_year\").setOutputCol(\"lemmatized\")\n",
    "\n",
    "# A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). \n",
    "#  Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words.\n",
    "ngrammer = NGramGenerator().setInputCols(['lemmatized']).setOutputCol('ngrams').setN(3).setEnableCumulative(True).setDelimiter('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline = Pipeline().setStages([\n",
    "                                    documentAssembler,\n",
    "                                    tokenizer,\n",
    "                                    normalizer,\n",
    "                                    removeYear,\n",
    "                                    lemmatizer,\n",
    "                                    ngrammer,\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization = training_pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenization.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                          lemmatized|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[{token, 1, 1, 5, {sentence -> 0}, []}, {token, 3, 7, star, {sentence -> 0}, []}, {token, 13, 16,...|\n",
      "|[{token, 1, 6, almost, {sentence -> 0}, []}, {token, 8, 17, everything, {sentence -> 0}, []}, {to...|\n",
      "|[{token, 1, 5, after, {sentence -> 0}, []}, {token, 7, 7, a, {sentence -> 0}, []}, {token, 9, 11,...|\n",
      "|[{token, 1, 5, after, {sentence -> 0}, []}, {token, 7, 8, i, {sentence -> 0}, []}, {token, 10, 12...|\n",
      "|[{token, 1, 5, after, {sentence -> 0}, []}, {token, 7, 11, year, {sentence -> 0}, []}, {token, 13...|\n",
      "|[{token, 1, 2, as, {sentence -> 0}, []}, {token, 4, 4, a, {sentence -> 0}, []}, {token, 6, 11, co...|\n",
      "|[{token, 1, 2, at, {sentence -> 0}, []}, {token, 4, 8, first, {sentence -> 0}, []}, {token, 11, 1...|\n",
      "|[{token, 1, 4, good, {sentence -> 0}, []}, {token, 6, 11, coffee, {sentence -> 0}, []}, {token, 1...|\n",
      "|[{token, 1, 7, call, {sentence -> 0}, []}, {token, 9, 12, this, {sentence -> 0}, []}, {token, 14,...|\n",
      "|[{token, 1, 6, coffee, {sentence -> 0}, []}, {token, 8, 12, maker, {sentence -> 0}, []}, {token, ...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.select(\"lemmatized\").show(10,truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                              ngrams|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[{chunk, 1, 1, 5, {sentence -> 0, chunk -> 0}, []}, {chunk, 3, 7, star, {sentence -> 0, chunk -> ...|\n",
      "|[{chunk, 1, 6, almost, {sentence -> 0, chunk -> 0}, []}, {chunk, 8, 17, everything, {sentence -> ...|\n",
      "|[{chunk, 1, 5, after, {sentence -> 0, chunk -> 0}, []}, {chunk, 7, 7, a, {sentence -> 0, chunk ->...|\n",
      "|[{chunk, 1, 5, after, {sentence -> 0, chunk -> 0}, []}, {chunk, 7, 8, i, {sentence -> 0, chunk ->...|\n",
      "|[{chunk, 1, 5, after, {sentence -> 0, chunk -> 0}, []}, {chunk, 7, 11, year, {sentence -> 0, chun...|\n",
      "|[{chunk, 1, 2, as, {sentence -> 0, chunk -> 0}, []}, {chunk, 4, 4, a, {sentence -> 0, chunk -> 1}...|\n",
      "|[{chunk, 1, 2, at, {sentence -> 0, chunk -> 0}, []}, {chunk, 4, 8, first, {sentence -> 0, chunk -...|\n",
      "|[{chunk, 1, 4, good, {sentence -> 0, chunk -> 0}, []}, {chunk, 6, 11, coffee, {sentence -> 0, chu...|\n",
      "|[{chunk, 1, 7, call, {sentence -> 0, chunk -> 0}, []}, {chunk, 9, 12, this, {sentence -> 0, chunk...|\n",
      "|[{chunk, 1, 6, coffee, {sentence -> 0, chunk -> 0}, []}, {chunk, 8, 12, maker, {sentence -> 0, ch...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_df.select(\"ngrams\").show(10,truncate=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
