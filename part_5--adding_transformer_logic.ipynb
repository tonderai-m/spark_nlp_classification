{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configspark import create_session, read_data\n",
    "import pyspark.sql.functions as f \n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Objective is to classify Reviews based on ratings to be honest you probably need to do chisquared first to see uniqueness of words per class\n",
    "  but that doesn't stop the fun this project is to show how to pipeline the data and some basic cleaning not trying to get the best model \n",
    "* The note books are separated in parts because I want to exime inside the pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configspark\n",
    "* I was being lazy and I added the spark config function and also the read data it's all in the configspark.py at list I added a schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/tonderaimadamba/spark_nlp_classification/spark_env/lib/python3.11/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/tonderaimadamba/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/tonderaimadamba/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1aa74600-ff1d-46c1-8405-d928a4f20609;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.0.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.15.0 in central\n",
      ":: resolution report :: resolve 548ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.0.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.15.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 by [com.google.errorprone#error_prone_annotations;2.18.0] in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   77  |   0   |   0   |   5   ||   72  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1aa74600-ff1d-46c1-8405-d928a4f20609\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 72 already retrieved (0kB/12ms)\n",
      "23/09/20 13:19:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = create_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data to train and validate \n",
    "* when you fit a pipeline I line to transform a different dataset than the one I have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data to train and validate \n",
    "* when you fit a pipeline I line to transform a different dataset than the one I have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Split the data using randomSplit()\n",
    "train_data, test_data, validation_data = df.randomSplit([train_ratio, test_ratio, validation_ratio], seed=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[review: string, rating: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelining\n",
    "* like it says pipeline it's a chain the previous transformation is linked to the next transformation order of excecution is maintained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to concatenate feature columns into one column called text\n",
    "# featureConcat = FeatureConcatenator(outputCols = [\"text\"], inputCols = [target_col])\n",
    "\n",
    "# Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. \n",
    "# The DocumentAssembler can read either a String column or an Array[String]\n",
    "documentAssembler = DocumentAssembler().setInputCol(\"review\").setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizes raw text in document type columns into TokenizedSentence\n",
    "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
    "\n",
    "# Annotator that cleans out tokens.\n",
    "# Remove white space\n",
    "normalizer = Normalizer().setInputCols(\"token\").setOutputCol(\"normalized\").setLowercase(True).setCleanupPatterns([\"[^\\w\\s]\"])\n",
    "\n",
    "# Remove years (integers starting with 19XX or 20XX)\n",
    "removeYear = Normalizer().setInputCols([\"normalized\"]).setOutputCol(\"remove_year\").setCleanupPatterns([\"(?:(?:19|20)\\d\\d)\"])\n",
    "\n",
    "# Find lemmas out of words with the objective of returning a base dictionary word\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols(\"remove_year\").setOutputCol(\"lemmatized\")\n",
    "\n",
    "# A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). \n",
    "#  Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words.\n",
    "ngrammer = NGramGenerator().setInputCols(['lemmatized']).setOutputCol('ngrams').setN(3).setEnableCumulative(True).setDelimiter('_')\n",
    "\n",
    "# Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. \n",
    "# The Finisher outputs annotation(s) values into String\n",
    "finisher = Finisher().setInputCols(['ngrams']).setOutputCols(['final'])\n",
    "\n",
    "# Maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "tfizer= HashingTF(inputCol='final', outputCol='tf_features')\n",
    "\n",
    "# Inverse document frequency\n",
    "# This implementation supports filtering out terms which do not appear in a minimum number of documents\n",
    "# idf = log((m + 1) / (d(t) + 1)), where m is the total number of documents and d(t) is the number of documents that contain term t.\n",
    "# the number of documents is the number of classes \n",
    "idfizer = IDF(inputCol='tf_features', outputCol=\"features\", minDocFreq = 2)\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol = \"rating\", outputCol = \"label\").fit(test_data) # we training with the test data because it's smaller this is proof of concerpt \n",
    "inverter = IndexToString(inputCol = \"prediction\", outputCol = \"predictionLabel\", labels = stringIndexer.labels)\n",
    "\n",
    "lr = LogisticRegression(elasticNetParam = 0.0, maxIter = 100, regParam =0.24781)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is just to show how to add transformer logic and class to pipeline easy steps\n",
    "* Another addition is to add UDF to this logic \n",
    "* The overall goal is to get read of the ml lib regressison library \n",
    "* It turns out that you can't really do that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer, Pipeline\n",
    "\n",
    "class ReplaceRLWithPandasPDF(Transformer):\n",
    "\n",
    "    def __init__(self, inputCol=\"predictionLabel\", outputCol=\"pandas_pdf\"):\n",
    "        super(ReplaceRLWithPandasPDF, self).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, df):\n",
    "        df = df.withColumn(self.outputCol, df[self.inputCol] + 10)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pipeline = Pipeline().setStages([\n",
    "                                    documentAssembler,\n",
    "                                    tokenizer,\n",
    "                                    normalizer,\n",
    "                                    removeYear,\n",
    "                                    lemmatizer,\n",
    "                                    ngrammer,\n",
    "                                    finisher,\n",
    "                                    tfizer,\n",
    "                                    idfizer,\n",
    "                                    stringIndexer,\n",
    "                                    lr,\n",
    "                                    inverter,\n",
    "                                    ReplaceRLWithPandasPDF()\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/20 13:19:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:19:58 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:01 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/09/20 13:20:01 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/09/20 13:20:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:03 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:04 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:05 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:07 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:08 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:11 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:13 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:14 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/09/20 13:20:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "model = train_pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+---------------+----------+\n",
      "|              review|rating|               final|         tf_features|            features|label|       rawPrediction|         probability|prediction|predictionLabel|pandas_pdf|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+---------------+----------+\n",
      "|\" I have been usi...|   5.0|[i, have, be, use...|(262144,[1227,131...|(262144,[1227,131...|  0.0|[2.26677347076030...|[0.79485195408169...|       0.0|            5.0|      15.0|\n",
      "|\"A great coffee m...|   5.0|[a, great, coffee...|(262144,[504,687,...|(262144,[504,687,...|  0.0|[2.58135611081055...|[0.85658202510186...|       0.0|            5.0|      15.0|\n",
      "|\"After only 6 mon...|   1.0|[after, only, 6, ...|(262144,[276,757,...|(262144,[276,757,...|  1.0|[-1.0462529313183...|[0.02891051848860...|       1.0|            1.0|      11.0|\n",
      "|\"Already not hot ...|   2.0|[already, not, ho...|(262144,[2306,269...|(262144,[2306,269...|  4.0|[1.31519041328996...|[0.55857403605696...|       0.0|            5.0|      15.0|\n",
      "|\"Great coffee pot...|   5.0|[great, coffee, p...|(262144,[141,2701...|(262144,[141,2701...|  0.0|[2.94120815141721...|[0.90620376736085...|       0.0|            5.0|      15.0|\n",
      "|\"I bought this co...|   2.0|[i, buy, this, co...|(262144,[1043,880...|(262144,[1043,880...|  4.0|[1.64233989275763...|[0.65277742792747...|       0.0|            5.0|      15.0|\n",
      "|\"I bought this co...|   5.0|[i, buy, this, co...|(262144,[353,1019...|(262144,[353,1019...|  0.0|[1.45462302715339...|[0.45621687241304...|       0.0|            5.0|      15.0|\n",
      "|\"I bought this mo...|   4.0|[i, buy, this, mo...|(262144,[409,1045...|(262144,[409,1045...|  2.0|[2.00113320076767...|[0.71724751185390...|       0.0|            5.0|      15.0|\n",
      "|\"I bought this to...|   4.0|[i, buy, this, to...|(262144,[633,2157...|(262144,[633,2157...|  2.0|[1.17485861068733...|[0.44982754512099...|       0.0|            5.0|      15.0|\n",
      "|\"I don't even dri...|   5.0|[i, dont, even, d...|(262144,[1211,914...|(262144,[1211,914...|  0.0|[1.99804837935824...|[0.74970507805679...|       0.0|            5.0|      15.0|\n",
      "|\"I hate coffee th...|   5.0|[i, hate, coffee,...|(262144,[1019,117...|(262144,[1019,117...|  0.0|[2.94422144636916...|[0.88226463685431...|       0.0|            5.0|      15.0|\n",
      "|\"I have to say, i...|   2.0|[i, have, to, say...|(262144,[1204,501...|(262144,[1204,501...|  4.0|[1.54310558527645...|[0.60581907237353...|       0.0|            5.0|      15.0|\n",
      "|\"I love the conce...|   3.0|[i, love, the, co...|(262144,[157,2088...|(262144,[157,2088...|  3.0|[-3.1035160410582...|[0.00423134774295...|       3.0|            3.0|      13.0|\n",
      "|\"I purchased this...|   5.0|[i, purchase, thi...|(262144,[1603,269...|(262144,[1603,269...|  0.0|[1.54678732857402...|[0.61473285589770...|       0.0|            5.0|      15.0|\n",
      "|\"I really love th...|   5.0|[i, really, love,...|(262144,[165,1319...|(262144,[165,1319...|  0.0|[2.08756902808691...|[0.73604378741499...|       0.0|            5.0|      15.0|\n",
      "|\"I've owned sever...|   5.0|[ive, own, severa...|(262144,[969,1019...|(262144,[969,1019...|  0.0|[0.04889547481870...|[0.16993032293224...|       1.0|            1.0|      11.0|\n",
      "|\"It has been a lo...|   1.0|[it, have, be, a,...|(262144,[2157,565...|(262144,[2157,565...|  1.0|[1.64351215760843...|[0.61948206227076...|       0.0|            5.0|      15.0|\n",
      "|\"Just purchased o...|   3.0|[just, purchase, ...|(262144,[1019,126...|(262144,[1019,126...|  3.0|[2.58211988318385...|[0.84535753759242...|       0.0|            5.0|      15.0|\n",
      "|\"Love it. I resea...|   5.0|[love, it, i, res...|(262144,[2437,536...|(262144,[2437,536...|  0.0|[2.37112322273620...|[0.78893335231578...|       0.0|            5.0|      15.0|\n",
      "|\"Love this! We sp...|   5.0|[love, this, we, ...|(262144,[2376,280...|(262144,[2376,280...|  0.0|[2.14419908704099...|[0.74686302299801...|       0.0|            5.0|      15.0|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/20 13:23:35 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/20 14:28:25 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 294422 ms exceeds timeout 120000 ms\n",
      "23/09/20 14:28:25 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|pandas_pdf|\n",
      "+----------+\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      11.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      13.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      11.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "|      15.0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/20 13:20:18 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n"
     ]
    }
   ],
   "source": [
    "predictions.select('pandas_pdf').show() ## We added 10 to prediction label meaning that 'prediction' is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
