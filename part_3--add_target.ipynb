{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configspark import create_session, read_data\n",
    "import pyspark.sql.functions as f \n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Objective is to classify Reviews based on ratings to be honest you probably need to do chisquared first to see uniqueness of words per class\n",
    "  but that doesn't stop the fun this project is to show how to pipeline the data and some basic cleaning not trying to get the best model \n",
    "* The note books are separated in parts because I want to exime inside the pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configspark\n",
    "* I was being lazy and I added the spark config function and also the read data it's all in the configspark.py at list I added a schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data to train and validate \n",
    "* when you fit a pipeline I line to transform a different dataset than the one I have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data to train and validate \n",
    "* when you fit a pipeline I line to transform a different dataset than the one I have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Split the data using randomSplit()\n",
    "train_data, test_data, validation_data = df.randomSplit([train_ratio, test_ratio, validation_ratio], seed=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[review: string, rating: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelining\n",
    "* like it says pipeline it's a chain the previous transformation is linked to the next transformation order of excecution is maintained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to concatenate feature columns into one column called text\n",
    "# featureConcat = FeatureConcatenator(outputCols = [\"text\"], inputCols = [target_col])\n",
    "\n",
    "# Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. \n",
    "# The DocumentAssembler can read either a String column or an Array[String]\n",
    "documentAssembler = DocumentAssembler().setInputCol(\"review\").setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizes raw text in document type columns into TokenizedSentence\n",
    "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
    "\n",
    "# Annotator that cleans out tokens.\n",
    "# Remove white space\n",
    "normalizer = Normalizer().setInputCols(\"token\").setOutputCol(\"normalized\").setLowercase(True).setCleanupPatterns([\"[^\\w\\s]\"])\n",
    "\n",
    "# Remove years (integers starting with 19XX or 20XX)\n",
    "removeYear = Normalizer().setInputCols([\"normalized\"]).setOutputCol(\"remove_year\").setCleanupPatterns([\"(?:(?:19|20)\\d\\d)\"])\n",
    "\n",
    "# Find lemmas out of words with the objective of returning a base dictionary word\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols(\"remove_year\").setOutputCol(\"lemmatized\")\n",
    "\n",
    "# A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). \n",
    "#  Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words.\n",
    "ngrammer = NGramGenerator().setInputCols(['lemmatized']).setOutputCol('ngrams').setN(3).setEnableCumulative(True).setDelimiter('_')\n",
    "\n",
    "# Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. \n",
    "# The Finisher outputs annotation(s) values into String\n",
    "finisher = Finisher().setInputCols(['ngrams']).setOutputCols(['final'])\n",
    "\n",
    "# Maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "tfizer= HashingTF(inputCol='final', outputCol='tf_features')\n",
    "\n",
    "# Inverse document frequency\n",
    "# This implementation supports filtering out terms which do not appear in a minimum number of documents\n",
    "# idf = log((m + 1) / (d(t) + 1)), where m is the total number of documents and d(t) is the number of documents that contain term t.\n",
    "# the number of documents is the number of classes \n",
    "idfizer = IDF(inputCol='tf_features', outputCol=\"features\", minDocFreq = 2)\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol = \"rating\", outputCol = \"label\").fit(train_data)\n",
    "inverter = IndexToString(inputCol = \"prediction\", outputCol = \"predictionLabel\", labels = stringIndexer.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCol = \"rating\", outputCol = \"label\").fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_Labels = stringIndexer.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----+\n",
      "|              review|rating|label|\n",
      "+--------------------+------+-----+\n",
      "|\"5 Stars - \"\"Very...|   5.0|  0.0|\n",
      "|\"ALMOST everythin...|   3.0|  4.0|\n",
      "|\"After a few week...|   4.0|  2.0|\n",
      "|\"After my old Ham...|   1.0|  1.0|\n",
      "|\"After years of u...|   5.0|  0.0|\n",
      "|\"As a coffee novi...|   5.0|  0.0|\n",
      "|\"At first, I like...|   2.0|  3.0|\n",
      "|\"Best coffee pot ...|   5.0|  0.0|\n",
      "|\"Calling this thi...|   1.0|  1.0|\n",
      "|\"Coffee maker lea...|   1.0|  1.0|\n",
      "|\"Coffee pot came ...|   1.0|  1.0|\n",
      "|\"Concerns with th...|   2.0|  3.0|\n",
      "|\"Easy and classic...|   5.0|  0.0|\n",
      "|\"Ever part of it ...|   1.0|  1.0|\n",
      "|\"Excellent coffee...|   5.0|  0.0|\n",
      "|\"Excellent produc...|   5.0|  0.0|\n",
      "|\"FIRST IMPRESSION...|   3.0|  4.0|\n",
      "|\"FULL DISCLOSURE:...|   5.0|  0.0|\n",
      "|\"First, understan...|   2.0|  3.0|\n",
      "|\"First-Nearly 6- ...|   5.0|  0.0|\n",
      "+--------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_Labels.show() ## The unique labels are just labels they mean nothing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
