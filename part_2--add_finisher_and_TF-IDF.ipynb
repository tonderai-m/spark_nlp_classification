{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configspark import create_session, read_data\n",
    "import pyspark.sql.functions as f \n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Objective is to classify Reviews based on ratings to be honest you probably need to do chisquared first to see uniqueness of words per class\n",
    "  but that doesn't stop the fun this project is to show how to pipeline the data and some basic cleaning not trying to get the best model \n",
    "* The note books are separated in parts because I want to exime inside the pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configspark\n",
    "* I was being lazy and I added the spark config function and also the read data it's all in the configspark.py at list I added a schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/tonderaimadamba/spark_nlp_classification/spark_env/lib/python3.11/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/tonderaimadamba/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/tonderaimadamba/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b3d0a40b-78e3-472c-806f-5441c1085da4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.0.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.15.0 in central\n",
      ":: resolution report :: resolve 547ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.0.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.15.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 by [com.google.errorprone#error_prone_annotations;2.18.0] in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   77  |   0   |   0   |   5   ||   72  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b3d0a40b-78e3-472c-806f-5441c1085da4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 72 already retrieved (0kB/12ms)\n",
      "23/09/12 23:18:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/12 23:18:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = create_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data to train and validate \n",
    "* when you fit a pipeline I line to transform a different dataset than the one I have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data to train and validate \n",
    "* when you fit a pipeline I line to transform a different dataset than the one I have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Split the data using randomSplit()\n",
    "train_data, test_data, validation_data = df.randomSplit([train_ratio, test_ratio, validation_ratio], seed=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[review: string, rating: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelining\n",
    "* like it says pipeline it's a chain the previous transformation is linked to the next transformation order of excecution is maintained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ / ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to concatenate feature columns into one column called text\n",
    "# featureConcat = FeatureConcatenator(outputCols = [\"text\"], inputCols = [target_col])\n",
    "\n",
    "# Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. \n",
    "# The DocumentAssembler can read either a String column or an Array[String]\n",
    "documentAssembler = DocumentAssembler().setInputCol(\"review\").setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizes raw text in document type columns into TokenizedSentence\n",
    "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
    "\n",
    "# Annotator that cleans out tokens.\n",
    "# Remove white space\n",
    "normalizer = Normalizer().setInputCols(\"token\").setOutputCol(\"normalized\").setLowercase(True).setCleanupPatterns([\"[^\\w\\s]\"])\n",
    "\n",
    "# Remove years (integers starting with 19XX or 20XX)\n",
    "removeYear = Normalizer().setInputCols([\"normalized\"]).setOutputCol(\"remove_year\").setCleanupPatterns([\"(?:(?:19|20)\\d\\d)\"])\n",
    "\n",
    "# Find lemmas out of words with the objective of returning a base dictionary word\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols(\"remove_year\").setOutputCol(\"lemmatized\")\n",
    "\n",
    "# A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). \n",
    "#  Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words.\n",
    "ngrammer = NGramGenerator().setInputCols(['lemmatized']).setOutputCol('ngrams').setN(3).setEnableCumulative(True).setDelimiter('_')\n",
    "\n",
    "# Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. \n",
    "# The Finisher outputs annotation(s) values into String\n",
    "finisher = Finisher().setInputCols(['ngrams']).setOutputCols(['final'])\n",
    "\n",
    "# Maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "tfizer= HashingTF(inputCol='final', outputCol='tf_features')\n",
    "\n",
    "# Inverse document frequency\n",
    "# This implementation supports filtering out terms which do not appear in a minimum number of documents\n",
    "# idf = log((m + 1) / (d(t) + 1)), where m is the total number of documents and d(t) is the number of documents that contain term t.\n",
    "# the number of documents is the number of classes \n",
    "idfizer = IDF(inputCol='tf_features', outputCol=\"features\", minDocFreq = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_pipeline = Pipeline().setStages([\n",
    "                                    documentAssembler,\n",
    "                                    tokenizer,\n",
    "                                    normalizer,\n",
    "                                    removeYear,\n",
    "                                    lemmatizer,\n",
    "                                    ngrammer,\n",
    "                                    finisher,\n",
    "                                    tfizer,\n",
    "                                    idfizer\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TF_IDF_convetor = TF_IDF_pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_df = TF_IDF_convetor.transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Measure of importance similar to the chisquared \n",
    "* It's all package with the Token and then the term frequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+\n",
      "|              review|rating|               final|         tf_features|            features|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+\n",
      "|\"5 Stars - \"\"Very...|   5.0|[5, star, very, h...|(262144,[44441,12...|(262144,[44441,12...|\n",
      "|\"ALMOST everythin...|   3.0|[almost, everythi...|(262144,[146,7036...|(262144,[146,7036...|\n",
      "|\"After a few week...|   4.0|[after, a, few, w...|(262144,[268,1110...|(262144,[268,1110...|\n",
      "|\"After my old Ham...|   1.0|[after, i, old, h...|(262144,[1147,268...|(262144,[1147,268...|\n",
      "|\"After years of u...|   5.0|[after, year, of,...|(262144,[150,2437...|(262144,[150,2437...|\n",
      "|\"As a coffee novi...|   5.0|[as, a, coffee, n...|(262144,[276,3353...|(262144,[276,3353...|\n",
      "|\"At first, I like...|   2.0|[at, first, i, li...|(262144,[221,752,...|(262144,[221,752,...|\n",
      "|\"Best coffee pot ...|   5.0|[good, coffee, po...|(262144,[351,660,...|(262144,[351,660,...|\n",
      "|\"Calling this thi...|   1.0|[call, this, thin...|(262144,[316,2157...|(262144,[316,2157...|\n",
      "|\"Coffee maker lea...|   1.0|[coffee, maker, l...|(262144,[1019,439...|(262144,[1019,439...|\n",
      "|\"Coffee pot came ...|   1.0|[coffee, pot, com...|(262144,[412,1757...|(262144,[412,1757...|\n",
      "|\"Concerns with th...|   2.0|[concern, with, t...|(262144,[2361,278...|(262144,[2361,278...|\n",
      "|\"Easy and classic...|   5.0|[easy, and, class...|(262144,[1319,131...|(262144,[1319,131...|\n",
      "|\"Ever part of it ...|   1.0|[ever, part, of, ...|(262144,[8804,119...|(262144,[8804,119...|\n",
      "|\"Excellent coffee...|   5.0|[excellent, coffe...|(262144,[333,2157...|(262144,[333,2157...|\n",
      "|\"Excellent produc...|   5.0|[excellent, produ...|(262144,[5676,702...|(262144,[5676,702...|\n",
      "|\"FIRST IMPRESSION...|   3.0|[first, impressio...|(262144,[6,1175,2...|(262144,[6,1175,2...|\n",
      "|\"FULL DISCLOSURE:...|   5.0|[full, disclosure...|(262144,[3276,491...|(262144,[3276,491...|\n",
      "|\"First, understan...|   2.0|[first, understan...|(262144,[2655,285...|(262144,[2655,285...|\n",
      "|\"First-Nearly 6- ...|   5.0|[firstnearly, 6, ...|(262144,[212,1019...|(262144,[212,1019...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 23:18:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                                features|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[44441,125422,144403,183411,190647,202791,210040,213314,245949],[6.795985378139127,6.795985378139127,0.0,3.291930611037264,4.46870767255471,4.398090105340756,1.8454540081125768,3.4031562461...|\n",
      "|(262144,[146,7036,12797,14541,20146,20345,20360,20833,21823,24056,25203,26465,27576,30546,30950,32890,33917,34996,35008,36271,42842,44167,46389,46547,48448,52879,54627,57279,67209,69176,70032,72303...|\n",
      "|(262144,[268,1110,1900,3011,3924,3928,5655,6033,6661,7549,8898,9056,9420,9884,10151,12472,12524,14002,15025,16898,17634,18575,19036,20995,21638,22421,24764,24980,25103,25261,27137,27576,28686,30950...|\n",
      "|(262144,[1147,2687,2761,4263,4291,4900,5330,5464,7625,8619,8972,9420,16029,16620,16898,18587,19036,19321,20524,20567,21549,23109,23496,23572,23644,24995,27576,29794,30950,31384,31626,31739,34465,35...|\n",
      "|(262144,[150,2437,2522,3824,4390,4689,5923,6455,6561,6620,6951,8282,8323,9420,9513,10072,10499,10819,11302,12083,12428,12524,13134,13993,15359,15494,15709,15888,16036,16337,16526,16592,17896,18176,...|\n",
      "|(262144,[276,3353,4254,4513,7823,8353,8804,10077,16898,18092,19036,25240,27576,37032,40123,40650,42404,48448,51219,55549,58672,61474,63466,64066,64316,67117,71597,78040,78198,85742,87531,92390,9242...|\n",
      "|(262144,[221,752,2325,2437,3255,3913,5163,5165,5558,8538,8972,9420,13855,15064,16863,17990,19036,19155,22346,22705,24111,24743,25113,25337,25505,25740,27576,28595,28985,30950,31015,32189,33860,3391...|\n",
      "|(262144,[351,660,4126,5923,7206,8113,11118,12519,13009,16337,16898,19036,21823,22158,27357,27576,27751,29977,30950,33113,34343,36231,37624,38844,40189,41332,43424,44923,46075,47197,48908,48982,5207...|\n",
      "|(262144,[316,2157,6266,6486,8619,9029,9420,10047,10263,12927,16898,18056,19036,20173,23979,26144,29772,30950,31066,32025,32105,33917,41359,42840,44106,44614,48410,48540,53184,55381,55942,58474,6139...|\n",
      "|(262144,[1019,4395,10697,11346,16408,16898,17718,18142,22631,23723,24598,27576,28760,32053,34042,34465,34712,42804,43509,44064,46389,53378,53562,55414,59381,63462,68957,69136,72646,77101,77247,8055...|\n",
      "|(262144,[412,1757,3912,5098,5634,7204,7336,7999,9183,9420,11552,12247,12524,16429,16933,17256,19036,19170,19678,20126,21165,21823,22169,22361,24645,24980,27576,27892,28595,29213,30267,30896,30950,3...|\n",
      "|(262144,[2361,2782,2810,5923,6561,6866,8439,8552,9420,9737,10292,19036,21192,21570,23206,25599,27213,27389,27576,30950,32240,32890,34360,34666,34860,36225,38600,43640,46767,46974,47197,48448,49175,...|\n",
      "|(262144,[1319,13134,16898,17280,19036,27492,27576,30604,30950,33212,34611,37733,39968,48448,51119,52553,52749,56921,57929,61112,61150,62809,70998,74382,74665,76969,94943,95889,96984,98717,100021,10...|\n",
      "|(262144,[8804,11946,24980,30950,37159,44864,44926,45638,49664,61961,66582,71656,74558,75994,80122,86649,86804,95889,98982,103196,103586,104944,105698,107107,111744,111998,117316,128891,134235,14363...|\n",
      "|(262144,[333,2157,3961,5381,7206,8439,10725,11999,12658,16036,16898,19036,20316,20567,21617,21823,23563,23921,24346,25203,25861,26144,26226,26892,27576,28296,30735,30950,30979,32663,33358,34343,395...|\n",
      "|(262144,[5676,7028,8867,9627,13799,13916,14561,16898,18587,18700,19352,20198,24222,24862,25451,27133,27576,28259,32250,34465,37328,38597,40965,42404,46926,48193,48557,50337,52879,55091,56372,62863,...|\n",
      "|(262144,[6,1175,2312,4214,4253,4343,5164,5239,7123,8538,8619,9164,9277,9420,10017,10452,10597,12524,13173,13565,13635,14479,16898,17358,18647,19036,19208,19770,19878,20420,21699,21823,22267,22983,2...|\n",
      "|(262144,[3276,4914,6113,13360,14376,16898,19036,30264,40034,40820,41910,43379,58988,59959,67091,68623,79419,87294,94127,95889,104636,106776,107107,107929,108541,109621,116631,117184,119762,123101,1...|\n",
      "|(262144,[2655,2855,3473,4133,4424,5381,5896,6034,6714,7123,7240,8439,8789,9370,12472,12805,13228,13636,14053,14671,14847,15501,16227,16898,17252,19036,19059,19287,20688,21570,21614,21617,21823,2256...|\n",
      "|(262144,[212,1019,1763,2280,2325,3491,3598,7625,9869,10867,11104,12524,15888,15998,16898,18054,18250,19036,19352,19533,19790,20475,25599,25667,26574,27476,27576,28668,30555,30835,30950,31156,31536,...|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 23:26:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_df.select(\"features\").show(truncate=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
