{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configspark import create_session, read_data\n",
    "import pyspark.sql.functions as f \n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Objective is to classify Reviews based on ratings to be honest you probably need to do chisquared first to see uniqueness of words per class\n",
    "  but that doesn't stop the fun this project is to show how to pipeline the data and some basic cleaning not trying to get the best model \n",
    "* The note books are separated in parts because I want to exime inside the pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Configspark\n",
    "* I was being lazy and I added the spark config function and also the read data it's all in the configspark.py at list I added a schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data to train and validate \n",
    "* when you fit a pipeline I line to transform a different dataset than the one I have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the data to train and validate \n",
    "* when you fit a pipeline I line to transform a different dataset than the one I have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Split the data using randomSplit()\n",
    "train_data, test_data, validation_data = df.randomSplit([train_ratio, test_ratio, validation_ratio], seed=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[review: string, rating: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelining\n",
    "* like it says pipeline it's a chain the previous transformation is linked to the next transformation order of excecution is maintained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to concatenate feature columns into one column called text\n",
    "# featureConcat = FeatureConcatenator(outputCols = [\"text\"], inputCols = [target_col])\n",
    "\n",
    "# Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. \n",
    "# The DocumentAssembler can read either a String column or an Array[String]\n",
    "documentAssembler = DocumentAssembler().setInputCol(\"review\").setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizes raw text in document type columns into TokenizedSentence\n",
    "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
    "\n",
    "# Annotator that cleans out tokens.\n",
    "# Remove white space\n",
    "normalizer = Normalizer().setInputCols(\"token\").setOutputCol(\"normalized\").setLowercase(True).setCleanupPatterns([\"[^\\w\\s]\"])\n",
    "\n",
    "# Remove years (integers starting with 19XX or 20XX)\n",
    "removeYear = Normalizer().setInputCols([\"normalized\"]).setOutputCol(\"remove_year\").setCleanupPatterns([\"(?:(?:19|20)\\d\\d)\"])\n",
    "\n",
    "# Find lemmas out of words with the objective of returning a base dictionary word\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols(\"remove_year\").setOutputCol(\"lemmatized\")\n",
    "\n",
    "# A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). \n",
    "#  Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words.\n",
    "ngrammer = NGramGenerator().setInputCols(['lemmatized']).setOutputCol('ngrams').setN(3).setEnableCumulative(True).setDelimiter('_')\n",
    "\n",
    "# Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. \n",
    "# The Finisher outputs annotation(s) values into String\n",
    "finisher = Finisher().setInputCols(['ngrams']).setOutputCols(['final'])\n",
    "\n",
    "# Maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "tfizer= HashingTF(inputCol='final', outputCol='tf_features')\n",
    "\n",
    "# Inverse document frequency\n",
    "# This implementation supports filtering out terms which do not appear in a minimum number of documents\n",
    "# idf = log((m + 1) / (d(t) + 1)), where m is the total number of documents and d(t) is the number of documents that contain term t.\n",
    "# the number of documents is the number of classes \n",
    "idfizer = IDF(inputCol='tf_features', outputCol=\"features\", minDocFreq = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_pipeline = Pipeline().setStages([\n",
    "                                    documentAssembler,\n",
    "                                    tokenizer,\n",
    "                                    normalizer,\n",
    "                                    removeYear,\n",
    "                                    lemmatizer,\n",
    "                                    ngrammer,\n",
    "                                    finisher,\n",
    "                                    tfizer,\n",
    "                                    idfizer\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "TF_IDF_convetor = TF_IDF_pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_df = TF_IDF_convetor.transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Measure of importance similar to the chisquared \n",
    "* It's all package with the Token and then the term frequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------------------+--------------------+\n",
      "|              review|rating|               final|         tf_features|            features|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+\n",
      "|\"5 Stars - \"\"Very...|   5.0|[5, star, very, h...|(262144,[44441,12...|(262144,[44441,12...|\n",
      "|\"ALMOST everythin...|   3.0|[almost, everythi...|(262144,[146,7036...|(262144,[146,7036...|\n",
      "|\"After a few week...|   4.0|[after, a, few, w...|(262144,[268,1110...|(262144,[268,1110...|\n",
      "|\"After my old Ham...|   1.0|[after, i, old, h...|(262144,[1147,268...|(262144,[1147,268...|\n",
      "|\"After years of u...|   5.0|[after, year, of,...|(262144,[150,2437...|(262144,[150,2437...|\n",
      "|\"As a coffee novi...|   5.0|[as, a, coffee, n...|(262144,[276,3353...|(262144,[276,3353...|\n",
      "|\"At first, I like...|   2.0|[at, first, i, li...|(262144,[221,752,...|(262144,[221,752,...|\n",
      "|\"Best coffee pot ...|   5.0|[good, coffee, po...|(262144,[351,660,...|(262144,[351,660,...|\n",
      "|\"Calling this thi...|   1.0|[call, this, thin...|(262144,[316,2157...|(262144,[316,2157...|\n",
      "|\"Coffee maker lea...|   1.0|[coffee, maker, l...|(262144,[1019,439...|(262144,[1019,439...|\n",
      "|\"Coffee pot came ...|   1.0|[coffee, pot, com...|(262144,[412,1757...|(262144,[412,1757...|\n",
      "|\"Concerns with th...|   2.0|[concern, with, t...|(262144,[2361,278...|(262144,[2361,278...|\n",
      "|\"Easy and classic...|   5.0|[easy, and, class...|(262144,[1319,131...|(262144,[1319,131...|\n",
      "|\"Ever part of it ...|   1.0|[ever, part, of, ...|(262144,[8804,119...|(262144,[8804,119...|\n",
      "|\"Excellent coffee...|   5.0|[excellent, coffe...|(262144,[333,2157...|(262144,[333,2157...|\n",
      "|\"Excellent produc...|   5.0|[excellent, produ...|(262144,[5676,702...|(262144,[5676,702...|\n",
      "|\"FIRST IMPRESSION...|   3.0|[first, impressio...|(262144,[6,1175,2...|(262144,[6,1175,2...|\n",
      "|\"FULL DISCLOSURE:...|   5.0|[full, disclosure...|(262144,[3276,491...|(262144,[3276,491...|\n",
      "|\"First, understan...|   2.0|[first, understan...|(262144,[2655,285...|(262144,[2655,285...|\n",
      "|\"First-Nearly 6- ...|   5.0|[firstnearly, 6, ...|(262144,[212,1019...|(262144,[212,1019...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 00:32:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                          features|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[44441,125422,144403,183411,190647,202791,210040,213314,245949],[6.795985378139127,6.795985378139127,0.0,3.291930611037...|\n",
      "|(262144,[146,7036,12797,14541,20146,20345,20360,20833,21823,24056,25203,26465,27576,30546,30950,32890,33917,34996,35008,36271,4...|\n",
      "|(262144,[268,1110,1900,3011,3924,3928,5655,6033,6661,7549,8898,9056,9420,9884,10151,12472,12524,14002,15025,16898,17634,18575,1...|\n",
      "|(262144,[1147,2687,2761,4263,4291,4900,5330,5464,7625,8619,8972,9420,16029,16620,16898,18587,19036,19321,20524,20567,21549,2310...|\n",
      "|(262144,[150,2437,2522,3824,4390,4689,5923,6455,6561,6620,6951,8282,8323,9420,9513,10072,10499,10819,11302,12083,12428,12524,13...|\n",
      "|(262144,[276,3353,4254,4513,7823,8353,8804,10077,16898,18092,19036,25240,27576,37032,40123,40650,42404,48448,51219,55549,58672,...|\n",
      "|(262144,[221,752,2325,2437,3255,3913,5163,5165,5558,8538,8972,9420,13855,15064,16863,17990,19036,19155,22346,22705,24111,24743,...|\n",
      "|(262144,[351,660,4126,5923,7206,8113,11118,12519,13009,16337,16898,19036,21823,22158,27357,27576,27751,29977,30950,33113,34343,...|\n",
      "|(262144,[316,2157,6266,6486,8619,9029,9420,10047,10263,12927,16898,18056,19036,20173,23979,26144,29772,30950,31066,32025,32105,...|\n",
      "|(262144,[1019,4395,10697,11346,16408,16898,17718,18142,22631,23723,24598,27576,28760,32053,34042,34465,34712,42804,43509,44064,...|\n",
      "|(262144,[412,1757,3912,5098,5634,7204,7336,7999,9183,9420,11552,12247,12524,16429,16933,17256,19036,19170,19678,20126,21165,218...|\n",
      "|(262144,[2361,2782,2810,5923,6561,6866,8439,8552,9420,9737,10292,19036,21192,21570,23206,25599,27213,27389,27576,30950,32240,32...|\n",
      "|(262144,[1319,13134,16898,17280,19036,27492,27576,30604,30950,33212,34611,37733,39968,48448,51119,52553,52749,56921,57929,61112...|\n",
      "|(262144,[8804,11946,24980,30950,37159,44864,44926,45638,49664,61961,66582,71656,74558,75994,80122,86649,86804,95889,98982,10319...|\n",
      "|(262144,[333,2157,3961,5381,7206,8439,10725,11999,12658,16036,16898,19036,20316,20567,21617,21823,23563,23921,24346,25203,25861...|\n",
      "|(262144,[5676,7028,8867,9627,13799,13916,14561,16898,18587,18700,19352,20198,24222,24862,25451,27133,27576,28259,32250,34465,37...|\n",
      "|(262144,[6,1175,2312,4214,4253,4343,5164,5239,7123,8538,8619,9164,9277,9420,10017,10452,10597,12524,13173,13565,13635,14479,168...|\n",
      "|(262144,[3276,4914,6113,13360,14376,16898,19036,30264,40034,40820,41910,43379,58988,59959,67091,68623,79419,87294,94127,95889,1...|\n",
      "|(262144,[2655,2855,3473,4133,4424,5381,5896,6034,6714,7123,7240,8439,8789,9370,12472,12805,13228,13636,14053,14671,14847,15501,...|\n",
      "|(262144,[212,1019,1763,2280,2325,3491,3598,7625,9869,10867,11104,12524,15888,15998,16898,18054,18250,19036,19352,19533,19790,20...|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 00:32:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "TF_IDF_df.select(\"features\").show(truncate=130)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
